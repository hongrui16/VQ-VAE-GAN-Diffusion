architecture:
  model_name: "vqvae" # vqgan, vqvae, vqvae_transformer, vqgan_transformer, vqdiffusion
  vqvae:
    img_channels: 3 # 1, 3
    img_size: 256
    latent_channels: 256
    latent_size: 16
    intermediate_channels: [128, 128, 256, 256, 512]
    num_residual_blocks_encoder: 2
    num_residual_blocks_decoder: 3
    dropout: 0.0
    attention_resolution: [16]
    num_codebook_vectors: 1024
    resume_path: None # '/home/rhong5/research_pro/hand_modeling_pro/pytorch-vqgan/log/Oxford102Flower/vqgan/run_2024-07-15-15-05-14/vqvae.pt'
    train_vqvae: True # True, False
    freeze_weights: True # True, False

  transformer:
    sos_token: 0
    pkeep: 0.5
    block_size: 512
    n_layer: 12
    n_head: 16
    n_embd: 1024
    resume_path: None
    train_transformer: True
    freeze_weights: False
  
  diffusion:
    diffusion_steps: 100
    sampling_steps: 100
    noise_schedule: "linear"
    diffusion_type: 'VQ_Official' #'VQ_Official', 'Continuous'
    objective: 'pred_noise'
    resume_path: None
    train_diffusion: False
    freeze_weights: False


dataset:
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
  batch_size: 20 # 20 for VAE, 100 for diffusion Unet2D,
  num_workers: 5
  dataset_name: 'InterHand26M' # mnist, cifar10, InterHand26M, Oxford102Flower
  get_hand_mask: True ### only for InterHand26M
  return_annotations: False
  

trainer:  
  num_epochs: 60
  log_dir: "log"
  
  vqvae:
    learning_rate: 2.25e-05
    beta1: 0.5
    beta2: 0.9
    perceptual_loss_factor: 1.0
    rec_loss_factor: 1.0
    perceptual_model: "vgg"

  transformer:
    learning_rate: 4.5e-06
    beta1: 0.9
    beta2: 0.95
  
  descriminator:
    disc_factor: 1.0
    disc_start: 100
    resume_path: None

  diffusion:
    learning_rate: 1.5e-04
    beta1: 0.6
    beta2: 0.95
    kl_loss_factor: 1.0
    rec_loss_factor: 1.0

