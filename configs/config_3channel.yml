architecture:
  model_name: "vqdiffusion" # vqgan, vqvae, vqvae_transformer, vqgan_transformer, vqdiffusion
  vqvae:
    img_channels: 1 # 1, 3
    img_size: 256
    latent_channels: 256
    latent_size: 16
    intermediate_channels: [128, 128, 256, 256, 512]
    num_residual_blocks_encoder: 2
    num_residual_blocks_decoder: 3
    dropout: 0.0
    attention_resolution: [16]
    num_codebook_vectors: 1024
    resume_path: None
    train_vqvae: False # True, False



  transformer:
    sos_token: 0
    pkeep: 0.5
    block_size: 512
    n_layer: 12
    n_head: 16
    n_embd: 1024
    resume_path: None
    train_transformer: True


  
  diffusion:
    diffusion_steps: 100
    sampling_steps: 100
    noise_schedule: "linear"
    diffusion_type: 'VQ_Official' #'VQ_Official', 'Continuous'
    objective: 'pred_noise'
    resume_path: None
    train_diffusion: True


trainer:
  batch_size: 1
  num_workers: 4
  num_epochs: 30
  log_dir: "log"
  dataset_name: 'mnist' # mnist, cifar10, InterHand26M, Oxford102Flower
  

  vqvae:
    learning_rate: 2.25e-05
    beta1: 0.5
    beta2: 0.9
    perceptual_loss_factor: 1.0
    rec_loss_factor: 1.0
    perceptual_model: "vgg"

  transformer:
    learning_rate: 4.5e-06
    beta1: 0.9
    beta2: 0.95
  
  descriminator:
    disc_factor: 1.0
    disc_start: 100
    resume_path: None

  diffusion:
    learning_rate: 1.5e-05
    beta1: 0.6
    beta2: 0.95
    kl_loss_factor: 1.0
    rec_loss_factor: 1.0
